import gc  # –°–±–æ—Ä—â–∏–∫ –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
import pickle

import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
from PIL import Image

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–†–ñ–Ø –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫", page_icon="üñêÔ∏è")

st.title("üñêÔ∏è –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ –∂–µ—Å—Ç–æ–≤ (–†–ñ–Ø)")
st.write("–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")


# --- –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 1: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
# @st.cache_resource –≥–æ–≤–æ—Ä–∏—Ç Streamlit: "–ó–∞–≥—Ä—É–∑–∏ —ç—Ç–æ –æ–¥–∏–Ω —Ä–∞–∑ –∏ –¥–µ—Ä–∂–∏ –≤ –ø–∞–º—è—Ç–∏"
@st.cache_resource
def load_model():
    try:
        model_dict = pickle.load(open("./model.p", "rb"))
        return model_dict["model"]
    except FileNotFoundError:
        return None


model = load_model()

if model is None:
    st.error("–û—à–∏–±–∫–∞: –§–∞–π–ª –º–æ–¥–µ–ª–∏ 'model.p' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    st.stop()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe (—Ç–æ–∂–µ –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏, –Ω–æ MP –ø–ª–æ—Ö–æ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–∏–º —Ç–∞–∫)
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)

# –í–∫–ª–∞–¥–∫–∏
tab1, tab2 = st.tabs(["üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–æ—Ç–æ", "üì∑ –°–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ"])

image_source = None

with tab1:
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª...", type=["jpg", "jpeg", "png"])
    if uploaded_file is not None:
        image_source = uploaded_file

with tab2:
    camera_file = st.camera_input("–°–¥–µ–ª–∞–π—Ç–µ —Å–Ω–∏–º–æ–∫")
    if camera_file is not None:
        image_source = camera_file

if image_source is not None:
    # 1. –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–æ—Ç–æ
    image = Image.open(image_source)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    if image_source == uploaded_file:
        st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ", use_container_width=True)

    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–∞—Å—Å–∏–≤
    img_array = np.array(image)

    if img_array.shape[-1] == 4:
        img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)

    # --- –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 2: –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ (Resize) ---
    # –ï—Å–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –±–æ–ª—å—à–µ 1000 –ø–∏–∫—Å–µ–ª–µ–π –ø–æ —à–∏—Ä–∏–Ω–µ, —É–º–µ–Ω—å—à–∞–µ–º –µ—ë
    # –≠—Ç–æ –ö–†–ò–¢–ò–ß–ï–°–ö–ò —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
    max_width = 800
    if img_array.shape[1] > max_width:
        scale_ratio = max_width / img_array.shape[1]
        new_height = int(img_array.shape[0] * scale_ratio)
        img_array = cv2.resize(img_array, (max_width, new_height))

    # 3. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    results = hands.process(img_array)

    if results.multi_hand_landmarks:
        data_aux = []
        x_ = []
        y_ = []

        hand_landmarks = results.multi_hand_landmarks[0]

        for i in range(len(hand_landmarks.landmark)):
            x = hand_landmarks.landmark[i].x
            y = hand_landmarks.landmark[i].y
            x_.append(x)
            y_.append(y)

        for i in range(len(hand_landmarks.landmark)):
            x = hand_landmarks.landmark[i].x
            y = hand_landmarks.landmark[i].y
            data_aux.append(x - min(x_))
            data_aux.append(y - min(y_))

        prediction = model.predict([np.asarray(data_aux)])
        predicted_character = prediction[0]

        st.markdown(
            f"""
        <div style="text-align: center; padding: 20px; background-color: #d4edda; border-radius: 10px; margin-top: 20px;">
            <h3 style="color: #155724;">–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π –∂–µ—Å—Ç:</h3>
            <h1 style="color: #155724; font-size: 72px; margin: 0;">{predicted_character}</h1>
        </div>
        """,
            unsafe_allow_html=True,
        )
    else:
        st.warning("‚ö†Ô∏è –†—É–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞.")

    # --- –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 3: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ ---
    del img_array
    del image
    del results
    gc.collect()
